import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
df=pd.read_csv('1.csv')
df.hist(bins=20,figsize=(12,10),edgecolor='black')
for col in df.columns:
    sns.boxplot(y=df[col])
    plt.show()
df.info()
df.describe()
numerical_features = df.select_dtypes(include=[np.number]).columns
for col in numerical_features:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
    print(f"{col} - Number of outliers: {len(outliers)}")
    
    
    
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('1.csv')
numeric_df = df.select_dtypes(include=['number'])
# Step 2: Compute the correlation matrix
correlation_matrix = numeric_df.corr()

# Step 3: Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of California Housing Features')
plt.show()

# Step 4: Create a pair plot to visualize pairwise relationships
sns.pairplot(df)
plt.suptitle('Pair Plot of California Housing Features', y=1.02)
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
ir = pd.read_csv('2.csv')
iris = datasets.load_iris()
X = iris.data  # Features (4D)
y = iris.target  # Target classes
X_scaled = StandardScaler().fit_transform(X)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.legend(labels=iris.target_names)
plt.show()
print(X_pca)
print(pca)
print(pca.explained_variance_ratio_)



import pandas as pd
def find_s_algorithm(file_path):
    data = pd.read_csv(file_path)
    print("Training data:")
    print(data)
    attributes = data.columns[:-1]
    class_label = data.columns[-1]
    hypothesis = ['?' for _ in attributes]
    for index, row in data.iterrows():
        if row[class_label] == 'Yes':
            for i, value in enumerate(row[attributes]):
                if hypothesis[i] == '?' or hypothesis[i] == value:
                    hypothesis[i] = value
                else:
                    hypothesis[i] = '?'
    return hypothesis
file_path = '4.csv'
hypothesis = find_s_algorithm(file_path)
print("\nThe final hypothesis is:", hypothesis)


import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

def knn(x_train, y_train, x_test, k):
    return [Counter(y_train[np.argsort(np.abs(x_train - t))[:k]]).most_common(1)[0][0] for t in x_test]

x = np.random.rand(100)
labels = np.where(x[:50] <= 0.5, 'Class1', 'Class2')

x_train, y_train, x_test = x[:50], labels[:50], x[50:]

for k in [1, 2, 4, 5, 8, 20, 30]:
    print(f'k={k}, Predictions: {knn(x_train, y_train, x_test, k)}')
    plt.scatter(x_train, np.zeros_like(x_train), c=['blue' if lbl == 'Class1' else 'red' for lbl in y_train])
    plt.scatter(x_test, np.zeros_like(x_test), marker='x', c='green')
    plt.show()



import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42)
X = np.linspace(-3, 3, 100).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.normal(scale=0.1, size=X.shape[0])
def locally_weighted_regression(x_query, X_train, y_train, tau):
    W = np.exp(-((X_train - x_query) ** 2) / (2 * tau ** 2))
    X_bias = np.c_[np.ones_like(X_train), X_train]
    theta = np.linalg.pinv(X_bias.T @ np.diag(W.ravel()) @ X_bias) @X_bias.T @ np.diag(W.ravel()) @ y_train
    return np.array([1, x_query]) @ theta
X_test = np.linspace(-3, 3, 100)
y_pred = np.array([locally_weighted_regression(x, X, y, tau=0.5) for x in X_test])
plt.scatter(X, y, color="gray", alpha=0.5, label="Training Data")
plt.plot(X_test, y_pred, color="red", linewidth=2, label="LWR Fit(Ï„=0.5)")
plt.legend()
plt.show()
  



from sklearn.datasets import fetch_openml 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error, r2_score 
boston = pd.read_csv('boston.csv')
X = boston[['RM']]  # Average number of rooms per dwelling
y = boston['MEDV']  # Median value of owner-occupied homes in $1000's
X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42) 
lr = LinearRegression() 
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print("\n--- Linear Regression (Boston Housing) ---") 
print("MSE:", mean_squared_error(y_test, y_pred)) 
print("R2 Score:", r2_score(y_test, y_pred))

plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')
plt.xlabel('Average number of rooms [RM]') 
plt.ylabel('House Price ($1000s)') 
plt.title('Linear Regression - Boston Housing') 
plt.legend() 
plt.show()

sorted_idx = X_test['RM'].argsort()
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test.iloc[sorted_idx], y_pred[sorted_idx], color='red', linewidth=2, label='Predicted') 
plt.title('Sorted Prediction Plot')
plt.xlabel('Average number of rooms [RM]')
plt.ylabel('House Price ($1000s)')
plt.legend()
plt.show()



from sklearn.preprocessing import PolynomialFeatures 
from sklearn.pipeline import make_pipeline 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error, r2_score 
import seaborn as sns 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
mpg = pd.read_csv('3.csv') 
mpg = mpg.dropna() 
X = mpg[['horsepower']] 
y = mpg['mpg'] 
X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42) 
degree = 2 
model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
model.fit(X_train, y_train) 
y_pred = model.predict(X_test) 
print("\n--- Polynomial Regression (Auto MPG) ---") 
print("MSE:", mean_squared_error(y_test, y_pred)) 
print("R2 Score:", r2_score(y_test, y_pred)) 

plt.scatter(X, y, color='blue', s=10, label='Actual') 
plt.plot(X_test, y_pred, color='red', linewidth=2, label=f'Degree  {degree} Fit') 
plt.xlabel('Horsepower') 
plt.ylabel('MPG (Fuel Efficiency)') 
plt.title('Polynomial Regression - Auto MPG') 
plt.legend() 
plt.show()
X_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1) 
y_range_pred = model.predict(X_range) 

plt.scatter(X, y, color='blue', s=10, label='Actual') 
plt.plot(X_range, y_range_pred, color='red', linewidth=2,  label=f'Degree {degree} Fit') 
plt.ylabel('MPG (Fuel Efficiency)') 
plt.title('Polynomial Regression - Auto MPG') 
plt.legend() 
plt.show()



# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.metrics import accuracy_score
from sklearn import tree
df=pd.read_csv('bc.csv')
X = df.drop(columns=['label'])
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
classifier = DecisionTreeClassifier(random_state=42) 
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)  # Compare predicted labels (y_pred) with actual labels (y_test)
print("Accuracy of the Decision Tree Classifier:", accuracy)
print("Decision Tree Structure:")
print(export_text(classifier, feature_names=list(X.columns)))
plt.figure(figsize=(40, 20), dpi=300)  # Bigger figure for clarity
plot_tree(classifier, feature_names=X.columns, filled=True, fontsize=8)
plt.show()



import numpy as np
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

data = fetch_olivetti_faces(shuffle=True, random_state=42,data_home="\home\something\scikit-learn')
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

gnb=GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=1))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

cross_val_accuracy = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')
print(f'\nCross-validation accuracy: {cross_val_accuracy.mean() * 100:.2f}%')

fig, axes = plt.subplots(3, 5, figsize=(12, 8))
for ax, image, label, prediction in zip(axes.ravel(), X_test, y_test, y_pred):
    ax.imshow(image.reshape(64, 64), cmap=plt.cm.gray)
    ax.set_title(f"True: {label}, Pred: {prediction}")
    ax.axis('off')
plt.show()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, classification_report
df = pd.read_csv('bc.csv')
data_features = df.drop(columns=['label'])
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_features)
data_scaled_df = pd.DataFrame(data_scaled, columns=data_features.columns)
data_scaled_df
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(data_scaled_df)
data['cluster'] = clusters
data[['target', 'cluster']]
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled_df)
pca_df = pd.DataFrame(data_pca, columns=['PCA1', 'PCA2'])
pca_df['cluster'] = clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=pca_df, palette='viridis')
plt.title('K-Means Clustering Results (PCA-Reduced Data)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.show()
